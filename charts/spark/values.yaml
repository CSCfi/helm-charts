## @param route Configuration for the Route resource in OpenShift
##
route:
  enabled: false
  host: ""
  tls:
    termination: edge
    insecureEdgeTerminationPolicy: Redirect

#########
# Spark #
#########
spark:
  global:
    security:
      allowInsecureImages: true

  image:
    repository: bitnamilegacy/spark
    tag: 4.0.0-debian-12-r20
  ## Spark master specific configuration
  ##
  master:
    ## @param master.configOptions Use a string to set the config options for in the form "-Dx=y"
    ##
    configOptions:
    ## @param master.extraEnvVars Extra environment variables to pass to the master container
    ## For example:
    ## extraEnvVars:
    ##  - name: SPARK_DAEMON_JAVA_OPTS
    ##    value: -Dx=y
    ##
      -Dspark.ui.reverseProxy=true
      -Dspark.ui.reverseProxyUrl=""
    ## @param master.resources Set container requests and limits for different resources like CPU or memory (essential for production workloads)
    ## Example:
    ## resources:
    ##   requests:
    ##     cpu: 2
    ##     memory: 512Mi
    ##   limits:
    ##     cpu: 3
    ##     memory: 1024Mi
    ##
    resources:
      requests:
        memory: 256Mi
        cpu: 250m
      limits:
        memory: 512Mi
        cpu: 500m

  ## Spark worker specific configuration
  ##
  worker:
    ## @param worker.configOptions Set extra options to configure the worker in the form `-Dx=y`
    ##
    configOptions:
    ## @param worker.extraEnvVars An array to add extra env vars
    ## For example:
    ## extraEnvVars:
    ##  - name: SPARK_DAEMON_JAVA_OPTS
    ##    value: -Dx=y
    ##
      -Dspark.ui.reverseProxy=true
      -Dspark.ui.reverseProxyUrl=""
    ## @param worker.replicaCount Number of spark workers (will be the minimum number when autoscaling is enabled)
    ##
    replicaCount: 2
    ## @param worker.resources Set container requests and limits for different resources like CPU or memory (essential for production workloads)
    ## Example:
    ## resources:
    ##   requests:
    ##     cpu: 2
    ##     memory: 512Mi
    ##   limits:
    ##     cpu: 3
    ##     memory: 1024Mi
    ##
    resources:
      requests:
        memory: 1G
        cpu: "1"
      limits:
        memory: 2G
        cpu: "2"
    ## Autoscaling parameters
    ## @param worker.autoscaling.enabled Enable replica autoscaling depending on CPU
    ## @param worker.autoscaling.minReplicas Minimum number of worker replicas
    ## @param worker.autoscaling.maxReplicas Maximum number of worker replicas
    ## @param worker.autoscaling.targetCPU Target CPU utilization percentage
    ## @param worker.autoscaling.targetMemory Target Memory utilization percentage
    ##
    autoscaling:
      enabled: false
      minReplicas: 1
      maxReplicas: 5
      targetCPU: 50
      targetMemory: ""
  
  ## Configure the ingress resource that allows you to access the
  ## Spark installation. Set up the URL
  ## ref: https://kubernetes.io/docs/concepts/services-networking/ingress/
  ##
  ingress:
    ## @param ingress.enabled Enable ingress controller resource
    ##
    enabled: true
    ## @param ingress.pathType Ingress path type
    ##
    pathType: ImplementationSpecific
    ## @param ingress.hostname Default host for the ingress resource
    ##
    hostname: ""
    ## @param ingress.path The Path to Spark. You may need to set this to '/*' in order to use this with ALB ingress controllers.
    ##
    path: /
    ## @param ingress.tls Enable TLS configuration for the hostname defined at ingress.hostname parameter
    ## TLS certificates will be retrieved from a TLS secret with name: {{- printf "%s-tls" .Values.ingress.hostname }}
    ## You can use the ingress.secrets parameter to create this TLS secret or rely on cert-manager to create it
    ##
    tls: false
